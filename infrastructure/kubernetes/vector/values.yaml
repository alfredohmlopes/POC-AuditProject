# Vector Helm Values
# https://helm.vector.dev
role: Aggregator

replicas: 3

customConfig:
  data_dir: "/vector-data-buffer"
  
  sources:
    http_ingest:
      type: "http_server"
      address: "0.0.0.0:8080"
      encoding: "json"
    
    # Kafka consumer from Redpanda for storage pipeline
    kafka_consumer:
      type: "kafka"
      bootstrap_servers: "redpanda.redpanda.svc.cluster.local:9093"
      group_id: "audit-consumer-group"
      topics: ["audit.events.v1"]
      auto_offset_reset: "earliest"
      decoding:
        codec: "json"
      sasl:
        enabled: true
        mechanism: "SCRAM-SHA-256"
        username: "turia-producer"
        password: "changeme_producer123"
    
    internal_metrics:
      type: "internal_metrics"

  transforms:
    validate:
      type: "remap"
      inputs: ["http_ingest"]
      source: |
        assert!(exists(.actor.id), "actor.id is required")
        assert!(exists(.action.name), "action.name is required")
        assert!(exists(.resource.type), "resource.type is required")
        assert!(exists(.resource.id), "resource.id is required")

    enrich:
      type: "remap"
      inputs: ["validate"]
      source: |
        .event_id = uuid_v7()
        .received_at = now()
        .event_date = format_timestamp!(.timestamp || now(), "%Y-%m-%d")
        .processing.vector_node = get_hostname!()
        .action.name = downcase(string!(.action.name))
        .tenant_id = .tenant_id || "default_tenant"
    
    # Prepare events from Kafka for storage - flatten nested objects for ClickHouse
    prepare_storage:
      type: "remap"
      inputs: ["kafka_consumer"]
      source: |
        # Flatten nested objects to match ClickHouse schema
        .event_date = .event_date || format_timestamp!(now(), "%Y-%m-%d")
        
        # Flatten actor object
        .actor_id = .actor.id || ""
        .actor_email = .actor.email || ""
        .actor_type = .actor.type || ""
        
        # Flatten action object
        .action_name = .action.name || ""
        
        # Flatten resource object
        .resource_type = .resource.type || ""
        .resource_id = .resource.id || ""
        
        # Flatten result object
        .result_success = .result.success || true
        .result_message = .result.reason || .result.message || ""
        
        # Flatten context object
        .context_ip = .context.ip || ""
        .context_user_agent = .context.user_agent || ""
        
        # Processing info
        .processing_vector_node = get_hostname!()
        
        # Store raw event for debugging
        .raw_event = encode_json(.)
        
        # Remove nested objects (ClickHouse expects flat columns)
        del(.actor)
        del(.action)
        del(.resource)
        del(.result)
        del(.context)
        del(.processing)
        del(.timestamp)
        del(.topic)
        del(.partition)
        del(.offset)

  sinks:
    # Sink to Prometheus Exporter for scraping
    prometheus_exporter:
      type: "prometheus_exporter"
      inputs: ["internal_metrics"]
      address: "0.0.0.0:9090"
    
    # Sink to Redpanda (Kafka) - Production
    redpanda:
      type: "kafka"
      inputs: ["enrich"]
      bootstrap_servers: "redpanda.redpanda.svc.cluster.local:9093"
      topic: "audit.events.v1"
      key_field: "tenant_id"
      compression: "lz4"
      encoding:
        codec: "json"
      batch:
        max_bytes: 1048576 # 1MB
        timeout_secs: 1
      acknowledgements:
        enabled: true
      sasl:
        enabled: true
        mechanism: "SCRAM-SHA-256"
        username: "turia-producer"
        password: "changeme_producer123"
    
    # ClickHouse Sink for Analytics (E3.S2)
    clickhouse:
      type: "clickhouse"
      inputs: ["prepare_storage"]
      endpoint: "http://clickhouse-audit-cluster.clickhouse.svc.cluster.local:8123?input_format_skip_unknown_fields=1"
      database: "audit"
      table: "events"
      auth:
        strategy: "basic"
        user: "writer"
        password: "changeme_writer123"
      encoding:
        timestamp_format: "rfc3339"
      skip_unknown_fields: true
      compression: "gzip"
      batch:
        max_events: 10000
        timeout_secs: 5
      request:
        retry_attempts: 3
        retry_initial_backoff_secs: 1
        retry_max_duration_secs: 10
    
    # OpenSearch Sink for Full-Text Search (E3.S3)
    opensearch:
      type: "elasticsearch"
      inputs: ["prepare_storage"]
      endpoints: ["https://audit-search.opensearch.svc.cluster.local:9200"]
      auth:
        strategy: "basic"
        user: "admin"
        password: "admin"
      tls:
        verify_certificate: false
      api_version: "v7"
      mode: "bulk"
      bulk:
        index: '{{ "{{" }} .event_date {{ "}}" }}'
        action: "index"
      batch:
        max_events: 5000
        timeout_secs: 5
      request:
        retry_attempts: 3
        retry_initial_backoff_secs: 1
        retry_max_duration_secs: 10
    
    # Debug console
    console_debug:
      type: "console"
      inputs: ["prepare_storage"]
      encoding:
        codec: "json"

persistence:
  enabled: true
  storageClassName: "standard" # or standard-ssd depending on cluster, trying standard first or empty for default
  accessModes:
    - ReadWriteOnce
  size: 1Gi

service:
  enabled: true
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
    - name: http-batch
      port: 8081
      targetPort: 8081
    - name: prom-exporter
      port: 9090
      targetPort: 9090

# Enable PodMonitor or ServiceMonitor if Prometheus Operator installed
serviceMonitor:
  enabled: true
  path: "/metrics"
  port: "prom-exporter"
